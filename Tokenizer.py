'''
-@author: Tyler Ray
-@date: 8/23/2023

- Tokenizes our input files
- ***WORK IN PROGRESS***

- Clarifier: I use github copilot, so some of the code may be generated by it. However most of the code is my ideas and my own work. I used it for most of the tedious work
'''
import re

token_specifications = {'symbols': r'\~|\@|\!|\$|\#|\^|\*|\%|\&|\(|\)|\[|\]|\{|\}|\<|\>|\+|\=|\_|\-|\||\/|\\|\;|\:|\'|\"|\,|\.|\?',
                            'double_symbols': r'\=\=|\<\=|\>\=|\!\=|\&\&|\|\||\/\/|\/\*|\*\/',
                            'types': r'int|float|char|void|double',
                            'characters': r'([a-zA-Z])',
                            'keywords': r'if|else|while|for|return|break|continue',
                            'identifiers': r'^[A-Za-z_][\w]*$',
                            'numbers': r'^[\d]+$'}

def type_detector(line, character, column_number):
    keyword = False
    skip = 0

    if character == 'i':
        if not len(line) <= column_number + 2:
            word = character + line[column_number + 1] + line[column_number + 2]

            if word == 'int':
                keyword = 'int'
                skip = 2
        
    elif character == 'c':
        if not len(line) <= column_number + 3:
            word = character + line[column_number + 1] + line[column_number + 2] + line[column_number + 3]

            if word == 'char':
                keyword = 'char'
                skip = 3
        
    elif character == 'v':
        if not len(line) <= column_number + 3:
            word = character + line[column_number + 1] + line[column_number + 2] + line[column_number + 3]

            if word == 'void':
                keyword = 'void'
                skip = 3
        
    elif character == 'f':
        if not len(line) <= column_number + 3:
            word = character + line[column_number + 1] + line[column_number + 2] + line[column_number + 3] + line[column_number + 4]

            if word == 'float':
                keyword = 'float'
                skip = 4
        
    elif character == 'd':
        if not len(line) <= column_number + 5:
            word = character + line[column_number + 1] + line[column_number + 2] + line[column_number + 3] + line[column_number + 4] + line[column_number + 5]

            if word == 'double':
                keyword = 'double'
                skip = 5
    
    
    if keyword != False:
        if line[column_number + skip + 1] == ' ': #if the type doesn't have a space before and after it isn't a type
            if (column_number - 1) >= 0:
                if line[column_number - 1] == ' ':
                    return keyword, skip # return the type and how many characters to skip
            else:
                return keyword, skip
            
    return False, 0 # return false if it is not a type
    
def keyword_detector(line, character, column_number):
    keyword = False
    skip = 0

    if character == 'i':
        if not len(line) <= column_number + 1: # Want to avoid OOB error
            word = character + line[column_number + 1]

            if word == 'if':
                keyword = 'if'
                skip = 1
        
    elif character == 'e':
        if not len(line) <= column_number + 3:
            word = character + line[column_number + 1] + line[column_number + 2] + line[column_number + 3]

            if word == 'else':
                keyword = 'else'
                skip = 3
        
    elif character == 'w':
        if not len(line) <= column_number + 4:
            word = character + line[column_number + 1] + line[column_number + 2] + line[column_number + 3] + line[column_number + 4]

            if word == 'while':
                keyword = 'while'
                skip = 4
        
    elif character == 'f':
        if not len(line) <= column_number + 3:
            word = character + line[column_number + 1] + line[column_number + 2]

            if word == 'for':
                keyword = 'for'
                skip = 3
        
    elif character == 'r':
        if not len(line) <= column_number + 6:
            word = character + line[column_number + 1] + line[column_number + 2] + line[column_number + 3] + line[column_number + 4] + line[column_number + 5]
            if word == 'return':
                keyword = 'return'
                skip = 6
        
    elif character == 'b':
        if not len(line) <= column_number + 4:
            word = character + line[column_number + 1] + line[column_number + 2] + line[column_number + 3] + line[column_number + 4]

            if word == 'break':
                keyword = 'break'
                skip = 4
        
    elif character == 'c':
        if not len(line) <= column_number + 8:
            word = character + line[column_number + 1] + line[column_number + 2] + line[column_number + 3] + line[column_number + 4] + line[column_number + 5] + line[column_number + 6] + line[column_number + 7]

            if word == 'continue':
                keyword = 'continue'
                skip = 8
    
    if keyword != False:
        if line[column_number + skip] == ' ':  # if the keyword doesn't have a space after it isn't a keyword
            if column_number - 1 >= 0:
                if line[column_number - 1] == ' ': #In the case this is a c one liner, we want to make sure the character before is a space or it is invalid
                    return keyword, skip
                
            else: #if the keyword is at the beginning of the line
                return keyword, skip
    return False, 0

def identifier_detector(line, character, column_number):
    word = character
    column_number += 1
    while (column_number <= (len(line) - 1)) and (not line[column_number] == ' '):
        word = word + line[column_number]
        column_number += 1

    if re.match(token_specifications['identifiers'], word): 
        skip_amount = len(word) - 1
        return word, skip_amount
    else: #If this doesn't match our format, we want to go back through the word and look for special chracters
        i = 0
        for i in range(len(word)):
            if not re.match(token_specifications['characters'], word[i]) and not re.match(token_specifications['numbers'], word[i]):
                skip_amount = i - 1
                return word[:i], skip_amount
    return False, 0
    
def character_adder(line, character, column_number, line_number, tokens, dictionaryIndex):
    skip = 0

    type_detection, skip_amount_type = type_detector(line, character, column_number) # check if the character is a type
    keyword_detection, skip_amount_keyword = keyword_detector(line, character, column_number) # check if the character is a keyword

    if type_detection != False: # if it is a type
        tokens[str(dictionaryIndex)] = ['type', type_detection, line_number, column_number]
        skip = skip_amount_type

    elif keyword_detection != False: # if it is a keyword
        tokens[str(dictionaryIndex)] = ['keyword', keyword_detection, line_number, column_number]
        skip = skip_amount_keyword

    else:

        identifier, skip_amount = identifier_detector(line, character, column_number) # check if the character is an identifier
        if identifier != False:
            tokens[str(dictionaryIndex)] = ['identifier', identifier, line_number, column_number]
            skip = skip_amount

    return tokens, dictionaryIndex, skip, column_number


def string_tokenizer(line, character, column_number, line_number, tokens, dictionaryIndex):

    skip = 0
    i = column_number + 1 #Don't care about first character since we already know it is a "
    word = ''
    while i < len(line):
        if line[i] == '"':
            break
        else:
            word = word + line[i]
            skip += 1
            i += 1

    dictionaryIndex += 1
    tokens[str(dictionaryIndex)] = ['string', word, line_number, column_number]

    dictionaryIndex += 1
    tokens[str(dictionaryIndex)] = ['symbols', '\"', line_number, column_number + skip + 1] # Go ahead and add the second quotation mark. 
    skip = skip + 1 #We want to skip over the second quotation mark

    return tokens, dictionaryIndex, skip

def symbol_adder(line, character, column_number, line_number, tokens, dictionaryIndex):
    skip = 0
    comment = False

    if len(line) <= column_number + 1: 
        tokens[str(dictionaryIndex)] = ['symbols', character, line_number, column_number]


    else:
        double_symbol = character + line[column_number + 1]
        if re.match(token_specifications['double_symbols'], double_symbol):
            tokens[str(dictionaryIndex)] = ['symbols', double_symbol, line_number, column_number]

            if double_symbol == '//': #Skip over comments, they aren't tokens and aren't necessary for our compiler
                skip = len(line) - column_number - 1
            elif double_symbol == '/*': 
                comment = True #don't calculate skip since we will be skipping entire lines
            else:
                skip = 1
            
        else: 
            tokens[str(dictionaryIndex)] = ['symbols', character, line_number, column_number]
            if character == '\"': #Tokenize strings
                tokens, dictionaryIndex, skip = string_tokenizer(line, character, column_number, line_number, tokens, dictionaryIndex)


    return tokens, dictionaryIndex, skip, column_number, comment

def main(input_file):
    
    tokens = {} #dictionary of tokens, the value is a array with the token and what line number it is found on
    dictionaryIndex = 0
    line_number = 0
    column_number = 0
    skip = 0
    comment = False

    for line in input_file:
        line = line.strip()
        line_number += 1
        column_number = 0
        for character in line:
            
            if comment == True: #If we are in a comment, we want to skip over the line. Used for multi line comments
               break

            if skip > 0:
                skip -= 1

            elif re.match(token_specifications['symbols'], character): #I moved this function out for readability
                tokens, dictionaryIndex, skip, column_number, comment = symbol_adder(line, character, column_number, line_number, tokens, dictionaryIndex)

            elif re.match(token_specifications['characters'], character): # if the character is a letter
                tokens, dictionaryIndex, skip, column_number = character_adder(line, character, column_number, line_number, tokens, dictionaryIndex)            

            dictionaryIndex += 1
            column_number += 1

        if '*/' in line and comment == True: #We do this after so we can skip over the final line
            tokens[str(dictionaryIndex)] = ['symbols', '*/', line_number, len(line) - 2]  #add the final */ to the tokens
            comment = False
            dictionaryIndex += 1

    print(tokens)
    return tokens
